{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Of Damages Caused By Floods & Earthqaukes Using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract\n",
    "\n",
    "Natural disasters are an increasing phenomenon clearly perceived and known to have a direct, life-altering impact on the welfare of the region it hits and its residents. Depending on where we live, hurricanes, earthquakes, floods, droughts, etc are a threat to lives, properties, productive assets & financial resources. The growing incidence of natural disasters is directly proportional to the increasing vulnerability of households and communities in affected regions. In this work, an artificial neural network has been used to predict the damages caused by natural disasters that can be felt at the community, city and state level as well as on an entire country. Artificial neural networks are mathematical models, inspired by a biological neural network process â€“ the biological neuron. They are used for the modeling of various complex input and output relationships as well as to find and match patterns of any given data. This report results in the comparison of different machine learning algorithms currently used to increase the accuracy of predictions. Training various neural networks, damages occurred due to floods & earthquakes have been estimated using test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "Natural disasters cause massive casualties, damages and leave many injured. Human beings cannot stop them but timely prediction and due safety measures can prevent loss of human lives and many precious objects can be saved. The main focus of this project is on the application of data-driven models in the context of real-time forecasting of the damages.\n",
    "\n",
    "This section follows an implementation plan which includes Data Selection, Data Preprocessing and Visualization, Application of Artificial Neural Networks, and its performance evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# External Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# Visualzation Packages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import squarify\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='psn1997', api_key='ffj08tmHIdZR3dUcbBIv')\n",
    "\n",
    "# Encoding Packages\n",
    "import category_encoders as ce  #Category Encoder\n",
    "from sklearn.preprocessing import LabelEncoder  #Label Encoder\n",
    "\n",
    "# Preprocessing Packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Machine Learning Models\n",
    "from sklearn import svm  #SVM Model\n",
    "from sklearn.tree import DecisionTreeClassifier  #Decision Tree Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier  #Random Forest Classifier\n",
    "\n",
    "# Artificial Neural Network Models\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense  #FFNN\n",
    "from keras.layers.recurrent import LSTM  #RNN\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from rbflayer import RBFLayer, InitCentersRandom  #RBFN \n",
    "\n",
    "# Evaluation Packages\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix Function \n",
    "\n",
    "A confusion matrix of size n x n associated with a classifier shows the predicted and actual classification, where n is the number of different classes. The prediction accuracy and classification error can be obtained from this matrix.\n",
    "This function prints and plots the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, normalize=False, title=None, cmap=plt.cm.Blues):\n",
    "    \n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation Function\n",
    "\n",
    "**Precision:** What proportion of positive identifications was actually correct.<br> \n",
    "**Recall:** What proportion of actual positives was identified correctly.<br>\n",
    "**F1 Score:** is needed when you want to seek a balance between Precision and Recall.<br>\n",
    "**Cohen Kappa Score:** Kappa Score is a metric that compares an Observed Accuracy with an Expected Accuracy.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, pred):\n",
    "    class_names = np.array(['0','1','2','3','4'])\n",
    "    plot_confusion_matrix(y_test, pred,classes= class_names , title='Confusion matrix, without normalization')\n",
    "    plt.show()\n",
    "    print(\"Cohen Kappa Score: \"+ str(cohen_kappa_score(y_test, pred)))\n",
    "    print(\"Classification report \\n\" + str(classification_report(y_test, pred, target_names=class_names)))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read Data\n",
    "\n",
    "National Centers for Environmental Information (formerly NGDC) maintains the data of significant earthquakes. It contains the information on destructive earthquakes from 2150 B.C. to present that meet at least one of the following criteria: Moderate Damage (approximately $1 million or more), 10 or more deaths, magnitude 7.5 or greater, or if earthquake generated a Tsunami.\n",
    "\n",
    "The portion of the dataset taken into consideration is from the year 1960 till 2019 and contains 2312 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earthquakes = pd.read_csv(\"Project/database.csv\", index_col=None)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing Date and Time\n",
    "earthquakes['DateTime_parsed'] = pd.to_datetime(earthquakes[['Month','Day','Year','Hour','Minute']], format = '%m/%d/%Y %I:%M %p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visulization of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_earthquakes = earthquakes.copy()\n",
    "viz_earthquakes = viz_earthquakes.dropna(subset=[\"Magnitude\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Map Plot\n",
    "World Map Damage vs Magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class = []\n",
    "\n",
    "for index, row in viz_earthquakes.iterrows():\n",
    "    if(row[\"Magnitude\"] < 5.0):\n",
    "        Class.append(\"Light\")\n",
    "    elif(row[\"Magnitude\"] >= 5.0 and row[\"Magnitude\"] < 7.0):\n",
    "        Class.append(\"Strong\")\n",
    "    elif(row[\"Magnitude\"] >= 7.0):\n",
    "        Class.append(\"Major\")\n",
    "    else:\n",
    "        Class.append(np.nan)\n",
    "        \n",
    "viz_earthquakes[\"Class\"] = Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = []\n",
    "\n",
    "for index, row in viz_earthquakes.iterrows():\n",
    "    if(row['Damage'] == 1.0):\n",
    "        size.append(600)\n",
    "    elif(row['Damage'] == 2.0):\n",
    "        size.append(800)\n",
    "    elif(row['Damage'] == 3.0):\n",
    "        size.append(900)\n",
    "    elif(row['Damage'] == 4.0):\n",
    "        size.append(1500)\n",
    "    else:\n",
    "        size.append(0)\n",
    "        \n",
    "viz_earthquakes[\"Size\"] = size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the dimension of the figure\n",
    "my_dpi=96\n",
    "plt.figure(figsize=(2600/my_dpi, 1800/my_dpi), dpi=my_dpi)\n",
    "\n",
    "# Make the background map\n",
    "m=Basemap(llcrnrlon=-180, llcrnrlat=-65,urcrnrlon=180,urcrnrlat=80)\n",
    "m.drawmapboundary(fill_color='#73DBFA', linewidth=0)\n",
    "m.fillcontinents(color='white', alpha=0.6)\n",
    "m.drawcoastlines(linewidth=0.1, color=\"white\")\n",
    "\n",
    "# prepare a color for each point depending on the continent.\n",
    "viz_earthquakes['labels_enc'] = pd.factorize(viz_earthquakes['Class'])[0]\n",
    " \n",
    "# Add a point per position\n",
    "colors = ['red','blue', 'green']\n",
    "m.scatter(viz_earthquakes['Longitude'], viz_earthquakes['Latitude'], s=viz_earthquakes['Size'], alpha=0.1, c=viz_earthquakes['labels_enc'], cmap=matplotlib.colors.ListedColormap(colors))\n",
    "\n",
    "# copyright and source data info\n",
    "plt.text( -170, -58,'Where people talk about #Surf\\n\\nData collected on twitter by @R_Graph_Gallery during 300 days\\nPlot realized with Python and the Basemap library', ha='left', va='bottom', size=15, color='#555555' )\n",
    " \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Squarify Plot <br>\n",
    "Earthquake Prone Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_earthquakes = viz_earthquakes['Country'].value_counts()\n",
    "freq_earthquakes = freq_earthquakes.to_frame().reset_index()\n",
    "freq_earthquakes = freq_earthquakes.rename(columns= {\"index\": \"Country\", \"Country\":\"Frequency\"})\n",
    "\n",
    "cmap = matplotlib.cm.Blues\n",
    "mini=freq_earthquakes['Frequency'][0:20].min()\n",
    "maxi=freq_earthquakes['Frequency'][0:20].max()\n",
    "norm = matplotlib.colors.Normalize(vmin=mini, vmax=maxi)\n",
    "colors = [cmap(norm(value)) for value in freq_earthquakes['Frequency'][0:20]]\n",
    "\n",
    "squarify.plot(sizes=freq_earthquakes['Frequency'][0:20], label=freq_earthquakes['Country'][0:20], alpha=.6, color=colors)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting a diagonal correlation matrix\n",
    "corr_earthquakes = viz_earthquakes[['Latitude', 'Longitude', 'Depth', 'Magnitude', 'Damage']].copy()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = corr_earthquakes.corr()\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing of Data\n",
    "**Discarding Null Values:** Preprocessing on earthquakes dataset involved discarding rows having null values, as these null values are either not supported by many machine learning models or these values caused the output to give a skewed accuracy. <br>\n",
    "**Scaling of Data:** Dataset comprised of attributes of varying scale and Latitude and Longitude attributes had negative coordinates. A few tree-based algorithms cannot take a negative value as an input hence scaling of these values was essential. It also helped in optimize the machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_earthquakes = earthquakes[['Latitude', 'Longitude', 'Depth', 'Magnitude', 'Damage', 'DateTime_parsed']].copy()\n",
    "final_earthquakes = final_earthquakes.dropna(subset=[\"Damage\"])\n",
    "final_earthquakes = final_earthquakes.dropna(subset=[\"Magnitude\"])\n",
    "final_earthquakes = final_earthquakes.dropna(subset=[\"Depth\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_earthquakes['TimeStamp'] = final_earthquakes['DateTime_parsed'].view('int64')\n",
    "final_earthquakes = final_earthquakes[['Latitude', 'Longitude', 'Depth', 'Magnitude', 'Damage']].copy()\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))\n",
    "svm_earthquakes = final_earthquakes.copy()\n",
    "svm_earthquakes[['Latitude', 'Longitude']] = scaler.fit_transform(svm_earthquakes[['Latitude', 'Longitude']])\n",
    "scaler = MinMaxScaler(feature_range=(0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Application of Models\n",
    "\n",
    "This project elaborates six models, which forms the core of our comprehensive comparative study to predict the possible damages caused due to natural disasters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_earthquakes[['Latitude', 'Longitude', 'Depth', 'Magnitude']].copy()\n",
    "y = final_earthquakes[['Damage']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Train & Test Data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "print(y_train['Damage'].value_counts())\n",
    "print(y_test['Damage'].value_counts())\n",
    "print(y_train['Damage'].value_counts(normalize = True))\n",
    "print(y_test['Damage'].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machine (SVM) Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SVM Model\n",
    "clf = svm.SVC(gamma='auto', decision_function_shape='ovo')\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "# Predict the response for test dataset\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Print Model Accuracy\n",
    "print(clf.score(X_test, y_test))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer\n",
    "clf = DecisionTreeClassifier(criterion=\"entropy\", splitter=\"best\",max_depth=9)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf = clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "pred = clf.predict(X_test)\n",
    "\n",
    "# Print Model Accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Random Forest classifer\n",
    "clf=RandomForestClassifier(n_estimators=200,criterion='entropy',max_depth=30,min_samples_split=10,min_samples_leaf=1, max_features='sqrt',verbose=1)\n",
    "\n",
    "# Train the model using the training sets\n",
    "clf.fit(X_train,y_train)\n",
    "\n",
    "# Predict the response for test dataset\n",
    "pred=clf.predict(X_test)\n",
    "\n",
    "# Print Model Accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, pred))\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Artificial Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covert classes to categorical type\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "encoded_Y = encoder.transform(y_train)\n",
    "\n",
    "encoded_y_train = np_utils.to_categorical(encoded_Y)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_test)\n",
    "encoded_Y = encoder.transform(y_test)\n",
    "\n",
    "encoded_y_test = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed Forward Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic Minority Over-sampling Technique\n",
    "\n",
    "\n",
    "smote = SMOTE('minority')\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "sm_X_train, sm_y_train = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "sm_X_train, sm_y_train = smote.fit_sample(sm_X_train, sm_y_train)\n",
    "\n",
    "print(sm_X_train.shape)\n",
    "print(sm_y_train.shape)\n",
    "\n",
    "print(\"1.0: \" + str(np.count_nonzero(sm_y_train == 1)))\n",
    "print(\"2.0: \" + str(np.count_nonzero(sm_y_train == 2)))\n",
    "print(\"3.0: \" + str(np.count_nonzero(sm_y_train == 3)))\n",
    "print(\"4.0: \" + str(np.count_nonzero(sm_y_train == 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assigning class weight\n",
    "a = y_train[\"Damage\"].values\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(a), a)\n",
    "print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Feed Forward Neural Network\n",
    "model = Sequential()\n",
    "model.add(Dense(16, activation='relu', input_dim=4))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the Network\n",
    "model.compile(loss='categorical_crossentropy', optimizer='SGD', metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, encoded_y_train, batch_size=10, epochs=40, verbose=1, class_weight = class_weight, validation_data=(X_test, encoded_y_test))\n",
    "\n",
    "# Print Model Accuracy\n",
    "[test_loss, test_acc] = model.evaluate(X_test, encoded_y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "pred = model.predict(X_test)+1\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recurrent Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Recurrent Neural Network\n",
    "embed_dim = 128\n",
    "lstm_out = 200\n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500, embed_dim,input_length = X.shape[1], dropout = 0.2))\n",
    "model.add(LSTM(lstm_out, dropout_U = 0.2, dropout_W = 0.2))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(4,activation='softmax'))\n",
    "\n",
    "# Compile the Network\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='SGD',metrics = ['accuracy'])\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X_train, encoded_y_train, batch_size=10, epochs=20, verbose=1, validation_data=(X_test, encoded_y_test))\n",
    "\n",
    "# Print Model Accuracy\n",
    "[test_loss, test_acc] = model.evaluate(X_test,encoded_y_test)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "pred = model.predict_classes(X_test)+1\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Radial Basis Function Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Radial Basis Function Network\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    X=X_train.values\n",
    "    y=encoded_y_train\n",
    "\n",
    "    model = Sequential()\n",
    "    rbflayer = RBFLayer(16,\n",
    "                        initializer=InitCentersRandom(X), \n",
    "                        betas=1.0,\n",
    "                        input_shape=(4,))\n",
    "    model.add(rbflayer)\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "# Compile the Network\n",
    "    model.compile(loss='mean_squared_error',\n",
    "                  optimizer=RMSprop(), metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the training sets\n",
    "    model.fit(X, y,\n",
    "              batch_size=10,\n",
    "              epochs=20,\n",
    "              verbose=1)\n",
    "\n",
    "    y_pred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Model Accuracy\n",
    "test_X=X_test.values\n",
    "test_Y=encoded_y_test\n",
    "[test_loss, test_acc] = model.evaluate(test_X,test_Y)\n",
    "print(\"Evaluation result on Test Data : Loss = {}, accuracy = {}\".format(test_loss, test_acc))\n",
    "pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate Model\n",
    "evaluate_model(y_test, pred)\n",
    "\n",
    "# Print Model Summary\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
